{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DjqbaQa4wD7g2qwusNrXR_Ak_xzMIf_D",
      "authorship_tag": "ABX9TyNE+zwW6PBFgXGVNp0VmzPY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eba0e8058a4544a499063de85028bbc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f8f72cf1d1e4b5690bc610e638dbed6",
              "IPY_MODEL_911d2d3a9dbe4a2a98a931141d976b94",
              "IPY_MODEL_71ece8a4875041d4826240f7c2e93164"
            ],
            "layout": "IPY_MODEL_535fdbd2399449afa7f980aeb49517a8"
          }
        },
        "9f8f72cf1d1e4b5690bc610e638dbed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c57bc94b0cc444aaf496efe8129998a",
            "placeholder": "​",
            "style": "IPY_MODEL_2579f89b0baa4338934f61357b92250c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "911d2d3a9dbe4a2a98a931141d976b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_742571fe2ff744e5bd3c1c1dbc3dac98",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3da3a5f2381c4cb5a2c08528b7cc683a",
            "value": 48
          }
        },
        "71ece8a4875041d4826240f7c2e93164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83d686cbe17b4e56b212492612416420",
            "placeholder": "​",
            "style": "IPY_MODEL_36c13c5da0e443e6b6d10ab1cb3e8c82",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.23kB/s]"
          }
        },
        "535fdbd2399449afa7f980aeb49517a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c57bc94b0cc444aaf496efe8129998a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2579f89b0baa4338934f61357b92250c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "742571fe2ff744e5bd3c1c1dbc3dac98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3da3a5f2381c4cb5a2c08528b7cc683a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83d686cbe17b4e56b212492612416420": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c13c5da0e443e6b6d10ab1cb3e8c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6283136b77749719500850655bdb6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a56a634ad20f44eca835571547192f90",
              "IPY_MODEL_41755d763c2249399f527102ef001c21",
              "IPY_MODEL_fbedbf0bef2143fa8e33e462b9e477e1"
            ],
            "layout": "IPY_MODEL_e0f6a48015774e1aa1bd69f5c3fe3315"
          }
        },
        "a56a634ad20f44eca835571547192f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c54a4db3246e4717a0ea676a42b66f31",
            "placeholder": "​",
            "style": "IPY_MODEL_2fdf4846d1b7447c8489a4f60bf313c2",
            "value": "config.json: 100%"
          }
        },
        "41755d763c2249399f527102ef001c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c021be37ae847afba1faedc0cd51d12",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9bb6d3e788c467290abafda4c5e7beb",
            "value": 570
          }
        },
        "fbedbf0bef2143fa8e33e462b9e477e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21fc9f3340e240a0ac38da5825dbb918",
            "placeholder": "​",
            "style": "IPY_MODEL_bd6253b3bbf34141b6ed15997757a95f",
            "value": " 570/570 [00:00&lt;00:00, 22.6kB/s]"
          }
        },
        "e0f6a48015774e1aa1bd69f5c3fe3315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c54a4db3246e4717a0ea676a42b66f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fdf4846d1b7447c8489a4f60bf313c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c021be37ae847afba1faedc0cd51d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9bb6d3e788c467290abafda4c5e7beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21fc9f3340e240a0ac38da5825dbb918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd6253b3bbf34141b6ed15997757a95f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15d484d8652e45fa8ee28b716af2b6eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c33fa6156af940059b24c87ab1e4453a",
              "IPY_MODEL_f399af8466e64de9b3d0483fb0158b60",
              "IPY_MODEL_5ab7a72f4ce3460da808003b8bbdf704"
            ],
            "layout": "IPY_MODEL_79d4f7cbe1214561bacc509f8f53a7aa"
          }
        },
        "c33fa6156af940059b24c87ab1e4453a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a0ef80fc4ef4d6b86e9fbf4c7f78572",
            "placeholder": "​",
            "style": "IPY_MODEL_ad4cb6c9409a4f1d8ef7aeeead52e62a",
            "value": "vocab.txt: 100%"
          }
        },
        "f399af8466e64de9b3d0483fb0158b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b640087f960743f99f91a1d89be52274",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_672262d9b39b45348571c4b667bf5851",
            "value": 231508
          }
        },
        "5ab7a72f4ce3460da808003b8bbdf704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6e75ba4a7804f3099f7fa77f2f2d1bb",
            "placeholder": "​",
            "style": "IPY_MODEL_301ada6710fb4a6c847355be4bcafe92",
            "value": " 232k/232k [00:00&lt;00:00, 5.74MB/s]"
          }
        },
        "79d4f7cbe1214561bacc509f8f53a7aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a0ef80fc4ef4d6b86e9fbf4c7f78572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad4cb6c9409a4f1d8ef7aeeead52e62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b640087f960743f99f91a1d89be52274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "672262d9b39b45348571c4b667bf5851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6e75ba4a7804f3099f7fa77f2f2d1bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301ada6710fb4a6c847355be4bcafe92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "587bb4d179e74de9b7da180a0e219f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79c4640a2ab14ed8a2e6ebc676f017df",
              "IPY_MODEL_745fb1354a354735adf68af9e78766e0",
              "IPY_MODEL_d5c44aa291a347b4b55f6bfbb258f257"
            ],
            "layout": "IPY_MODEL_d35312967e2b4f5e86c2b81ad187e574"
          }
        },
        "79c4640a2ab14ed8a2e6ebc676f017df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cb0cbabec294e19aa53bc38c917bb15",
            "placeholder": "​",
            "style": "IPY_MODEL_bcf296aa80204b5891c634b6f0e2e134",
            "value": "tokenizer.json: 100%"
          }
        },
        "745fb1354a354735adf68af9e78766e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db4b7564f1924846a538f8a6b94d6cb6",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_841b4897920b472b9b59466adc4dca04",
            "value": 466062
          }
        },
        "d5c44aa291a347b4b55f6bfbb258f257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39cd703e0ed24c1f82cefd8564cf372b",
            "placeholder": "​",
            "style": "IPY_MODEL_b8cab99b0df34841af98f6f98a0a9c6f",
            "value": " 466k/466k [00:00&lt;00:00, 3.09MB/s]"
          }
        },
        "d35312967e2b4f5e86c2b81ad187e574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb0cbabec294e19aa53bc38c917bb15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcf296aa80204b5891c634b6f0e2e134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db4b7564f1924846a538f8a6b94d6cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841b4897920b472b9b59466adc4dca04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39cd703e0ed24c1f82cefd8564cf372b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8cab99b0df34841af98f6f98a0a9c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namra-3305/PaperSense-Intelligent-Document-Platform/blob/main/src/data_processing/cnn_dailymail_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # src/data_processing/cnn_dailymail_preprocessing.py\n",
        "# \"\"\"\n",
        "# CNN/DailyMail Dataset Preprocessing\n",
        "# Handles text summarization data preparation\n",
        "# \"\"\"\n",
        "\n",
        "# import os\n",
        "# import json\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from datasets import load_dataset, load_from_disk\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from transformers import AutoTokenizer\n",
        "# import nltk\n",
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "# import re\n",
        "# from collections import Counter\n",
        "# from tqdm import tqdm\n",
        "# import pickle\n",
        "\n",
        "# # Download required NLTK data\n",
        "# nltk.download('punkt_tab', quiet=True)\n",
        "# nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# # Project paths - FIXED\n",
        "# PROJECT_ROOT = \"/content/drive/MyDrive/PaperSense-Intelligent-Document-Platform\"\n",
        "# RAW_DATA_DIR = os.path.join(PROJECT_ROOT, \"datasets\", \"raw\", \"cnn_dailymail\")\n",
        "# PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT, \"datasets\", \"processed\", \"cnn_dailymail\")\n",
        "# RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\", \"visualizations\", \"generation\")\n",
        "\n",
        "# def ensure_dir(path):\n",
        "#     \"\"\"Create directory if it doesn't exist\"\"\"\n",
        "#     os.makedirs(path, exist_ok=True)\n",
        "#     return path\n",
        "\n",
        "# def download_cnn_dailymail_dataset():\n",
        "#     \"\"\"Download and save CNN/DailyMail dataset\"\"\"\n",
        "#     print(\"📥 Downloading CNN/DailyMail dataset...\")\n",
        "\n",
        "#     try:\n",
        "#         # Download dataset from Hugging Face\n",
        "#         dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "#         # Create directory and save\n",
        "#         ensure_dir(os.path.dirname(RAW_DATA_DIR))\n",
        "#         dataset.save_to_disk(RAW_DATA_DIR)\n",
        "\n",
        "#         print(f\"✅ Dataset downloaded and saved to: {RAW_DATA_DIR}\")\n",
        "#         return dataset\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Error downloading dataset: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def load_cnn_dailymail_dataset():\n",
        "#     \"\"\"Load CNN/DailyMail dataset from disk or download if not exists\"\"\"\n",
        "#     print(\"📂 Loading CNN/DailyMail dataset...\")\n",
        "\n",
        "#     try:\n",
        "#         # Try to load from disk first\n",
        "#         if os.path.exists(RAW_DATA_DIR) and os.listdir(RAW_DATA_DIR):\n",
        "#             dataset = load_from_disk(RAW_DATA_DIR)\n",
        "#             print(f\"✅ Dataset loaded from disk with splits: {list(dataset.keys())}\")\n",
        "#         else:\n",
        "#             print(\"📥 Dataset not found on disk, downloading...\")\n",
        "#             dataset = download_cnn_dailymail_dataset()\n",
        "#             if dataset is None:\n",
        "#                 return None\n",
        "\n",
        "#         # Print basic info\n",
        "#         for split_name, split_data in dataset.items():\n",
        "#             print(f\"  {split_name}: {len(split_data)} samples\")\n",
        "\n",
        "#         return dataset\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Error loading dataset: {e}\")\n",
        "#         print(\"🔄 Attempting to download dataset...\")\n",
        "#         return download_cnn_dailymail_dataset()\n",
        "\n",
        "# def analyze_text_statistics(dataset):\n",
        "#     \"\"\"Analyze text length and content statistics\"\"\"\n",
        "#     print(\"📊 Analyzing text statistics...\")\n",
        "\n",
        "#     ensure_dir(RESULTS_DIR)\n",
        "\n",
        "#     # Collect statistics\n",
        "#     article_lengths = []\n",
        "#     summary_lengths = []\n",
        "#     article_sent_counts = []\n",
        "#     summary_sent_counts = []\n",
        "\n",
        "#     # Sample from train set for analysis\n",
        "#     train_data = dataset['train']\n",
        "#     sample_size = min(10000, len(train_data))  # Sample for faster analysis\n",
        "\n",
        "#     # Fix: Convert numpy integers to Python integers\n",
        "#     sample_indices = np.random.choice(len(train_data), sample_size, replace=False)\n",
        "#     sample_indices = [int(idx) for idx in sample_indices]  # Convert to Python int\n",
        "\n",
        "#     print(f\"Analyzing {sample_size} samples from training set...\")\n",
        "\n",
        "#     for idx in tqdm(sample_indices, desc=\"Analyzing samples\"):\n",
        "#         article = train_data[idx]['article']\n",
        "#         summary = train_data[idx]['highlights']\n",
        "\n",
        "#         # Word counts\n",
        "#         article_words = len(word_tokenize(article))\n",
        "#         summary_words = len(word_tokenize(summary))\n",
        "#         article_lengths.append(article_words)\n",
        "#         summary_lengths.append(summary_words)\n",
        "\n",
        "#         # Sentence counts\n",
        "#         article_sents = len(sent_tokenize(article))\n",
        "#         summary_sents = len(sent_tokenize(summary))\n",
        "#         article_sent_counts.append(article_sents)\n",
        "#         summary_sent_counts.append(summary_sents)\n",
        "\n",
        "#     # Create visualizations\n",
        "#     fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "#     # Article length distribution\n",
        "#     axes[0,0].hist(article_lengths, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "#     axes[0,0].set_title('Article Length Distribution (Words)')\n",
        "#     axes[0,0].set_xlabel('Number of Words')\n",
        "#     axes[0,0].set_ylabel('Frequency')\n",
        "#     axes[0,0].axvline(np.mean(article_lengths), color='red', linestyle='--',\n",
        "#                      label=f'Mean: {np.mean(article_lengths):.0f}')\n",
        "#     axes[0,0].legend()\n",
        "\n",
        "#     # Summary length distribution\n",
        "#     axes[0,1].hist(summary_lengths, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "#     axes[0,1].set_title('Summary Length Distribution (Words)')\n",
        "#     axes[0,1].set_xlabel('Number of Words')\n",
        "#     axes[0,1].set_ylabel('Frequency')\n",
        "#     axes[0,1].axvline(np.mean(summary_lengths), color='red', linestyle='--',\n",
        "#                      label=f'Mean: {np.mean(summary_lengths):.0f}')\n",
        "#     axes[0,1].legend()\n",
        "\n",
        "#     # Length ratio distribution\n",
        "#     ratios = [s/a if a > 0 else 0 for a, s in zip(article_lengths, summary_lengths)]\n",
        "#     axes[0,2].hist(ratios, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
        "#     axes[0,2].set_title('Summary/Article Length Ratio')\n",
        "#     axes[0,2].set_xlabel('Ratio')\n",
        "#     axes[0,2].set_ylabel('Frequency')\n",
        "#     axes[0,2].axvline(np.mean(ratios), color='red', linestyle='--',\n",
        "#                      label=f'Mean: {np.mean(ratios):.3f}')\n",
        "#     axes[0,2].legend()\n",
        "\n",
        "#     # Article sentence count\n",
        "#     axes[1,0].hist(article_sent_counts, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "#     axes[1,0].set_title('Article Sentence Count Distribution')\n",
        "#     axes[1,0].set_xlabel('Number of Sentences')\n",
        "#     axes[1,0].set_ylabel('Frequency')\n",
        "#     axes[1,0].axvline(np.mean(article_sent_counts), color='red', linestyle='--',\n",
        "#                      label=f'Mean: {np.mean(article_sent_counts):.1f}')\n",
        "#     axes[1,0].legend()\n",
        "\n",
        "#     # Summary sentence count\n",
        "#     axes[1,1].hist(summary_sent_counts, bins=15, alpha=0.7, color='brown', edgecolor='black')\n",
        "#     axes[1,1].set_title('Summary Sentence Count Distribution')\n",
        "#     axes[1,1].set_xlabel('Number of Sentences')\n",
        "#     axes[1,1].set_ylabel('Frequency')\n",
        "#     axes[1,1].axvline(np.mean(summary_sent_counts), color='red', linestyle='--',\n",
        "#                      label=f'Mean: {np.mean(summary_sent_counts):.1f}')\n",
        "#     axes[1,1].legend()\n",
        "\n",
        "#     # Correlation plot\n",
        "#     axes[1,2].scatter(article_lengths, summary_lengths, alpha=0.5, s=1)\n",
        "#     axes[1,2].set_title('Article vs Summary Length')\n",
        "#     axes[1,2].set_xlabel('Article Length (Words)')\n",
        "#     axes[1,2].set_ylabel('Summary Length (Words)')\n",
        "\n",
        "#     # Add correlation coefficient\n",
        "#     correlation = np.corrcoef(article_lengths, summary_lengths)[0,1]\n",
        "#     axes[1,2].text(0.05, 0.95, f'Correlation: {correlation:.3f}',\n",
        "#                   transform=axes[1,2].transAxes, verticalalignment='top',\n",
        "#                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(RESULTS_DIR, 'text_statistics.png'), dpi=300, bbox_inches='tight')\n",
        "#     plt.close()\n",
        "\n",
        "#     # Print statistics\n",
        "#     stats = {\n",
        "#         'article_length': {\n",
        "#             'mean': np.mean(article_lengths),\n",
        "#             'std': np.std(article_lengths),\n",
        "#             'median': np.median(article_lengths),\n",
        "#             'min': np.min(article_lengths),\n",
        "#             'max': np.max(article_lengths)\n",
        "#         },\n",
        "#         'summary_length': {\n",
        "#             'mean': np.mean(summary_lengths),\n",
        "#             'std': np.std(summary_lengths),\n",
        "#             'median': np.median(summary_lengths),\n",
        "#             'min': np.min(summary_lengths),\n",
        "#             'max': np.max(summary_lengths)\n",
        "#         },\n",
        "#         'compression_ratio': {\n",
        "#             'mean': np.mean(ratios),\n",
        "#             'std': np.std(ratios),\n",
        "#             'median': np.median(ratios)\n",
        "#         }\n",
        "#     }\n",
        "\n",
        "#     print(f\"📈 Text Statistics:\")\n",
        "#     print(f\"  Article length - Mean: {stats['article_length']['mean']:.1f}, \"\n",
        "#           f\"Median: {stats['article_length']['median']:.1f}, \"\n",
        "#           f\"Range: {stats['article_length']['min']}-{stats['article_length']['max']}\")\n",
        "#     print(f\"  Summary length - Mean: {stats['summary_length']['mean']:.1f}, \"\n",
        "#           f\"Median: {stats['summary_length']['median']:.1f}, \"\n",
        "#           f\"Range: {stats['summary_length']['min']}-{stats['summary_length']['max']}\")\n",
        "#     print(f\"  Compression ratio - Mean: {stats['compression_ratio']['mean']:.3f}, \"\n",
        "#           f\"Median: {stats['compression_ratio']['median']:.3f}\")\n",
        "\n",
        "#     return stats\n",
        "\n",
        "# def clean_text(text):\n",
        "#     \"\"\"Clean and normalize text\"\"\"\n",
        "#     # Remove extra whitespace\n",
        "#     text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "#     # Remove special characters but keep punctuation\n",
        "#     text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', '', text)\n",
        "\n",
        "#     # Fix common issues\n",
        "#     text = text.replace(' .', '.')\n",
        "#     text = text.replace(' ,', ',')\n",
        "#     text = text.replace('( ', '(')\n",
        "#     text = text.replace(' )', ')')\n",
        "\n",
        "#     return text.strip()\n",
        "\n",
        "# def create_vocabulary_analysis(dataset, max_vocab_size=50000):\n",
        "#     \"\"\"Analyze vocabulary and create word frequency statistics\"\"\"\n",
        "#     print(\"📚 Creating vocabulary analysis...\")\n",
        "\n",
        "#     ensure_dir(RESULTS_DIR)\n",
        "#     ensure_dir(PROCESSED_DATA_DIR)  # Ensure this exists for vocabulary.json\n",
        "\n",
        "#     # Collect text from training set\n",
        "#     train_data = dataset['train']\n",
        "#     sample_size = min(10000, len(train_data))  # Sample for faster analysis\n",
        "\n",
        "#     # Fix: Convert numpy integers to Python integers\n",
        "#     sample_indices = np.random.choice(len(train_data), sample_size, replace=False)\n",
        "#     sample_indices = [int(idx) for idx in sample_indices]  # Convert to Python int\n",
        "\n",
        "#     all_words = []\n",
        "#     stopwords_en = set(stopwords.words('english'))\n",
        "\n",
        "#     print(f\"Analyzing vocabulary from {sample_size} samples...\")\n",
        "\n",
        "#     for idx in tqdm(sample_indices, desc=\"Processing vocabulary\"):\n",
        "#         article = train_data[idx]['article']\n",
        "#         summary = train_data[idx]['highlights']\n",
        "\n",
        "#         # Tokenize and clean\n",
        "#         article_words = word_tokenize(clean_text(article.lower()))\n",
        "#         summary_words = word_tokenize(clean_text(summary.lower()))\n",
        "\n",
        "#         # Filter words\n",
        "#         article_words = [w for w in article_words if w.isalpha() and w not in stopwords_en]\n",
        "#         summary_words = [w for w in summary_words if w.isalpha() and w not in stopwords_en]\n",
        "\n",
        "#         all_words.extend(article_words)\n",
        "#         all_words.extend(summary_words)\n",
        "\n",
        "#     # Create word frequency distribution\n",
        "#     word_freq = Counter(all_words)\n",
        "#     most_common = word_freq.most_common(max_vocab_size)\n",
        "\n",
        "#     # Visualizations\n",
        "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "#     # Top 30 words\n",
        "#     top_30_words = most_common[:30]\n",
        "#     words, counts = zip(*top_30_words)\n",
        "\n",
        "#     ax1.barh(range(len(words)), counts)\n",
        "#     ax1.set_yticks(range(len(words)))\n",
        "#     ax1.set_yticklabels(words)\n",
        "#     ax1.set_title('Top 30 Most Frequent Words')\n",
        "#     ax1.set_xlabel('Frequency')\n",
        "#     ax1.invert_yaxis()\n",
        "\n",
        "#     # Word frequency distribution (Zipf's law)\n",
        "#     ranks = range(1, min(1000, len(most_common)) + 1)\n",
        "#     freqs = [count for _, count in most_common[:len(ranks)]]\n",
        "\n",
        "#     ax2.loglog(ranks, freqs, 'b-', alpha=0.7)\n",
        "#     ax2.set_title('Word Frequency Distribution (Zipf\\'s Law)')\n",
        "#     ax2.set_xlabel('Rank')\n",
        "#     ax2.set_ylabel('Frequency')\n",
        "#     ax2.grid(True, alpha=0.3)\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(RESULTS_DIR, 'vocabulary_analysis.png'), dpi=300, bbox_inches='tight')\n",
        "#     plt.close()\n",
        "\n",
        "#     # Save vocabulary\n",
        "#     vocab_info = {\n",
        "#         'total_words': len(all_words),\n",
        "#         'unique_words': len(word_freq),\n",
        "#         'most_common': most_common[:1000],  # Save top 1000\n",
        "#         'vocabulary_size': len(word_freq)\n",
        "#     }\n",
        "\n",
        "#     with open(os.path.join(PROCESSED_DATA_DIR, 'vocabulary.json'), 'w') as f:\n",
        "#         json.dump(vocab_info, f, indent=2)\n",
        "\n",
        "#     print(f\"  Total words processed: {len(all_words):,}\")\n",
        "#     print(f\"  Unique words: {len(word_freq):,}\")\n",
        "#     print(f\"  Most common word: '{most_common[0][0]}' ({most_common[0][1]} occurrences)\")\n",
        "\n",
        "#     return vocab_info\n",
        "\n",
        "# def tokenize_and_preprocess(dataset, tokenizer_name='bert-base-uncased', max_samples=None):\n",
        "#     \"\"\"Tokenize text using a pre-trained tokenizer\"\"\"\n",
        "#     print(f\"🔤 Tokenizing text with {tokenizer_name}...\")\n",
        "\n",
        "#     try:\n",
        "#         tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "#     except:\n",
        "#         print(f\"Warning: Could not load {tokenizer_name}, using basic preprocessing\")\n",
        "#         return preprocess_basic_tokenization(dataset, max_samples)\n",
        "\n",
        "#     processed_splits = {}\n",
        "\n",
        "#     for split_name, split_data in dataset.items():\n",
        "#         print(f\"Processing {split_name} split...\")\n",
        "\n",
        "#         # Limit samples for testing\n",
        "#         if max_samples and split_name == 'train':\n",
        "#             indices = list(range(min(max_samples, len(split_data))))  # Convert to list of Python ints\n",
        "#         else:\n",
        "#             indices = list(range(len(split_data)))  # Convert to list of Python ints\n",
        "\n",
        "#         articles = []\n",
        "#         summaries = []\n",
        "\n",
        "#         for idx in tqdm(indices, desc=f\"Tokenizing {split_name}\"):\n",
        "#             article = clean_text(split_data[idx]['article'])\n",
        "#             summary = clean_text(split_data[idx]['highlights'])\n",
        "\n",
        "#             # Tokenize article (truncate to model max length)\n",
        "#             article_tokens = tokenizer(\n",
        "#                 article,\n",
        "#                 max_length=512,\n",
        "#                 truncation=True,\n",
        "#                 padding=False,\n",
        "#                 return_tensors=\"pt\"\n",
        "#             )\n",
        "\n",
        "#             # Tokenize summary\n",
        "#             summary_tokens = tokenizer(\n",
        "#                 summary,\n",
        "#                 max_length=128,\n",
        "#                 truncation=True,\n",
        "#                 padding=False,\n",
        "#                 return_tensors=\"pt\"\n",
        "#             )\n",
        "\n",
        "#             articles.append({\n",
        "#                 'input_ids': article_tokens['input_ids'].squeeze(),\n",
        "#                 'attention_mask': article_tokens['attention_mask'].squeeze(),\n",
        "#                 'text': article\n",
        "#             })\n",
        "\n",
        "#             summaries.append({\n",
        "#                 'input_ids': summary_tokens['input_ids'].squeeze(),\n",
        "#                 'attention_mask': summary_tokens['attention_mask'].squeeze(),\n",
        "#                 'text': summary\n",
        "#             })\n",
        "\n",
        "#         processed_splits[split_name] = {\n",
        "#             'articles': articles,\n",
        "#             'summaries': summaries,\n",
        "#             'count': len(articles)\n",
        "#         }\n",
        "\n",
        "#         print(f\"✅ {split_name}: {len(articles)} samples processed\")\n",
        "\n",
        "#     return processed_splits, tokenizer\n",
        "\n",
        "# def preprocess_basic_tokenization(dataset, max_samples=None):\n",
        "#     \"\"\"Basic text preprocessing without transformer tokenizer\"\"\"\n",
        "#     print(\"🔤 Performing basic text preprocessing...\")\n",
        "\n",
        "#     processed_splits = {}\n",
        "\n",
        "#     for split_name, split_data in dataset.items():\n",
        "#         print(f\"Processing {split_name} split...\")\n",
        "\n",
        "#         # Limit samples for testing\n",
        "#         if max_samples and split_name == 'train':\n",
        "#             indices = list(range(min(max_samples, len(split_data))))  # Convert to list of Python ints\n",
        "#         else:\n",
        "#             indices = list(range(len(split_data)))  # Convert to list of Python ints\n",
        "\n",
        "#         articles = []\n",
        "#         summaries = []\n",
        "\n",
        "#         for idx in tqdm(indices, desc=f\"Processing {split_name}\"):\n",
        "#             article = clean_text(split_data[idx]['article'])\n",
        "#             summary = clean_text(split_data[idx]['highlights'])\n",
        "\n",
        "#             # Basic tokenization\n",
        "#             article_words = word_tokenize(article.lower())\n",
        "#             summary_words = word_tokenize(summary.lower())\n",
        "\n",
        "#             articles.append({\n",
        "#                 'tokens': article_words,\n",
        "#                 'text': article,\n",
        "#                 'length': len(article_words)\n",
        "#             })\n",
        "\n",
        "#             summaries.append({\n",
        "#                 'tokens': summary_words,\n",
        "#                 'text': summary,\n",
        "#                 'length': len(summary_words)\n",
        "#             })\n",
        "\n",
        "#         processed_splits[split_name] = {\n",
        "#             'articles': articles,\n",
        "#             'summaries': summaries,\n",
        "#             'count': len(articles)\n",
        "#         }\n",
        "\n",
        "#         print(f\"✅ {split_name}: {len(articles)} samples processed\")\n",
        "\n",
        "#     return processed_splits, None\n",
        "\n",
        "# def save_processed_data(processed_splits, tokenizer, text_stats, vocab_info):\n",
        "#     \"\"\"Save processed CNN/DailyMail data\"\"\"\n",
        "#     print(\"💾 Saving processed data...\")\n",
        "\n",
        "#     ensure_dir(PROCESSED_DATA_DIR)\n",
        "\n",
        "#     # Save processed splits\n",
        "#     for split_name, split_data in processed_splits.items():\n",
        "#         with open(os.path.join(PROCESSED_DATA_DIR, f\"{split_name}_processed.pkl\"), 'wb') as f:\n",
        "#             pickle.dump(split_data, f)\n",
        "\n",
        "#     # Save tokenizer if available\n",
        "#     if tokenizer:\n",
        "#         tokenizer.save_pretrained(os.path.join(PROCESSED_DATA_DIR, 'tokenizer'))\n",
        "\n",
        "#     # Save metadata\n",
        "#     metadata = {\n",
        "#         'dataset_name': 'CNN/DailyMail',\n",
        "#         'task': 'text_summarization',\n",
        "#         'splits': {name: data['count'] for name, data in processed_splits.items()},\n",
        "#         'tokenizer': tokenizer.name_or_path if tokenizer else 'basic_tokenization',\n",
        "#         'text_statistics': text_stats,\n",
        "#         'vocabulary_info': {\n",
        "#             'total_words': vocab_info['total_words'],\n",
        "#             'unique_words': vocab_info['unique_words'],\n",
        "#             'vocabulary_size': vocab_info['vocabulary_size']\n",
        "#         },\n",
        "#         'preprocessing_info': {\n",
        "#             'max_article_length': 512,\n",
        "#             'max_summary_length': 128,\n",
        "#             'text_cleaning': True,\n",
        "#             'truncation': True\n",
        "#         }\n",
        "#     }\n",
        "\n",
        "#     with open(os.path.join(PROCESSED_DATA_DIR, 'metadata.json'), 'w') as f:\n",
        "#         json.dump(metadata, f, indent=2)\n",
        "\n",
        "#     print(f\"✅ Data saved to {PROCESSED_DATA_DIR}\")\n",
        "\n",
        "#     # Print summary\n",
        "#     total_samples = sum(data['count'] for data in processed_splits.values())\n",
        "#     print(f\"\\n📋 Processing Summary:\")\n",
        "#     print(f\"  Total samples: {total_samples:,}\")\n",
        "#     for split_name, data in processed_splits.items():\n",
        "#         print(f\"  {split_name}: {data['count']:,} samples\")\n",
        "\n",
        "# def create_sample_examples(processed_splits, num_examples=5):\n",
        "#     \"\"\"Create sample examples for visualization\"\"\"\n",
        "#     print(\"📝 Creating sample examples...\")\n",
        "\n",
        "#     ensure_dir(RESULTS_DIR)\n",
        "\n",
        "#     train_data = processed_splits['train']\n",
        "\n",
        "#     # Fix: Convert numpy integers to Python integers\n",
        "#     sample_indices = np.random.choice(len(train_data['articles']), num_examples, replace=False)\n",
        "#     sample_indices = [int(idx) for idx in sample_indices]  # Convert to Python int\n",
        "\n",
        "#     examples = []\n",
        "#     for idx in sample_indices:\n",
        "#         article = train_data['articles'][idx]\n",
        "#         summary = train_data['summaries'][idx]\n",
        "\n",
        "#         example = {\n",
        "#             'index': idx,\n",
        "#             'article': article['text'][:1000] + '...' if len(article['text']) > 1000 else article['text'],\n",
        "#             'summary': summary['text'],\n",
        "#             'article_length': len(article['text'].split()) if 'text' in article else article.get('length', 0),\n",
        "#             'summary_length': len(summary['text'].split()) if 'text' in summary else summary.get('length', 0)\n",
        "#         }\n",
        "#         examples.append(example)\n",
        "\n",
        "#     # Save examples as JSON for easy viewing\n",
        "#     with open(os.path.join(RESULTS_DIR, 'sample_examples.json'), 'w') as f:\n",
        "#         json.dump(examples, f, indent=2)\n",
        "\n",
        "#     # Create a formatted text file\n",
        "#     with open(os.path.join(RESULTS_DIR, 'sample_examples.txt'), 'w') as f:\n",
        "#         f.write(\"CNN/DailyMail Dataset - Sample Examples\\n\")\n",
        "#         f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "#         for i, example in enumerate(examples, 1):\n",
        "#             f.write(f\"EXAMPLE {i}:\\n\")\n",
        "#             f.write(f\"Article Length: {example['article_length']} words\\n\")\n",
        "#             f.write(f\"Summary Length: {example['summary_length']} words\\n\")\n",
        "#             f.write(f\"Compression Ratio: {example['summary_length']/example['article_length']:.3f}\\n\\n\")\n",
        "\n",
        "#             f.write(\"ARTICLE:\\n\")\n",
        "#             f.write(example['article'])\n",
        "#             f.write(\"\\n\\nSUMMARY:\\n\")\n",
        "#             f.write(example['summary'])\n",
        "#             f.write(\"\\n\\n\" + \"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "#     print(f\"✅ {num_examples} sample examples saved to {RESULTS_DIR}\")\n",
        "#     return examples\n",
        "\n",
        "# def main(max_samples=None, tokenizer_name='bert-base-uncased', create_visualizations=True):\n",
        "#     \"\"\"Main CNN/DailyMail preprocessing function\"\"\"\n",
        "#     print(\"🚀 Starting CNN/DailyMail Preprocessing...\")\n",
        "#     print(\"=\" * 50)\n",
        "\n",
        "#     # Create directories\n",
        "#     ensure_dir(PROCESSED_DATA_DIR)\n",
        "#     ensure_dir(RESULTS_DIR)\n",
        "\n",
        "#     # Load dataset (will download if not exists)\n",
        "#     dataset = load_cnn_dailymail_dataset()\n",
        "#     if dataset is None:\n",
        "#         print(\"❌ Failed to load dataset. Please check your internet connection.\")\n",
        "#         return False\n",
        "\n",
        "#     try:\n",
        "#         # Analyze text statistics\n",
        "#         if create_visualizations:\n",
        "#             print(\"\\n📊 Analyzing text statistics...\")\n",
        "#             text_stats = analyze_text_statistics(dataset)\n",
        "\n",
        "#             print(\"\\n📚 Analyzing vocabulary...\")\n",
        "#             vocab_info = create_vocabulary_analysis(dataset)\n",
        "#         else:\n",
        "#             text_stats = {}\n",
        "#             vocab_info = {'total_words': 0, 'unique_words': 0, 'vocabulary_size': 0}\n",
        "\n",
        "#         # Tokenize and preprocess\n",
        "#         print(\"\\n🔤 Tokenizing and preprocessing...\")\n",
        "#         processed_splits, tokenizer = tokenize_and_preprocess(\n",
        "#             dataset, tokenizer_name, max_samples\n",
        "#         )\n",
        "\n",
        "#         # Create sample examples\n",
        "#         if create_visualizations and processed_splits:\n",
        "#             create_sample_examples(processed_splits)\n",
        "\n",
        "#         # Save processed data\n",
        "#         if processed_splits:\n",
        "#             save_processed_data(processed_splits, tokenizer, text_stats, vocab_info)\n",
        "#             print(\"🎉 CNN/DailyMail preprocessing completed successfully!\")\n",
        "#             return True\n",
        "#         else:\n",
        "#             print(\"❌ No data was processed!\")\n",
        "#             return False\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Error during preprocessing: {e}\")\n",
        "#         import traceback\n",
        "#         traceback.print_exc()\n",
        "#         return False\n",
        "\n",
        "# # Quick download function for immediate use\n",
        "# def quick_download():\n",
        "#     \"\"\"Quick function to just download the dataset\"\"\"\n",
        "#     print(\"🚀 Quick Download: CNN/DailyMail Dataset\")\n",
        "#     ensure_dir(os.path.dirname(RAW_DATA_DIR))\n",
        "#     dataset = download_cnn_dailymail_dataset()\n",
        "#     return dataset is not None\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # For testing, limit samples per split\n",
        "#     # Remove max_samples parameter for full processing\n",
        "#     success = main(\n",
        "#         max_samples=1000,  # Limit for testing - set to None for full dataset\n",
        "#         tokenizer_name='bert-base-uncased',\n",
        "#         create_visualizations=True\n",
        "#     )\n",
        "\n",
        "#     if success:\n",
        "#         print(\"\\n✅ Preprocessing completed! You can now use the processed data for training.\")\n",
        "#         print(f\"📁 Raw data: {RAW_DATA_DIR}\")\n",
        "#         print(f\"📁 Processed data: {PROCESSED_DATA_DIR}\")\n",
        "#         print(f\"📁 Visualizations: {RESULTS_DIR}\")\n",
        "#     else:\n",
        "#         print(\"\\n❌ Preprocessing failed! Check the error messages above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eba0e8058a4544a499063de85028bbc3",
            "9f8f72cf1d1e4b5690bc610e638dbed6",
            "911d2d3a9dbe4a2a98a931141d976b94",
            "71ece8a4875041d4826240f7c2e93164",
            "535fdbd2399449afa7f980aeb49517a8",
            "6c57bc94b0cc444aaf496efe8129998a",
            "2579f89b0baa4338934f61357b92250c",
            "742571fe2ff744e5bd3c1c1dbc3dac98",
            "3da3a5f2381c4cb5a2c08528b7cc683a",
            "83d686cbe17b4e56b212492612416420",
            "36c13c5da0e443e6b6d10ab1cb3e8c82",
            "c6283136b77749719500850655bdb6ed",
            "a56a634ad20f44eca835571547192f90",
            "41755d763c2249399f527102ef001c21",
            "fbedbf0bef2143fa8e33e462b9e477e1",
            "e0f6a48015774e1aa1bd69f5c3fe3315",
            "c54a4db3246e4717a0ea676a42b66f31",
            "2fdf4846d1b7447c8489a4f60bf313c2",
            "9c021be37ae847afba1faedc0cd51d12",
            "c9bb6d3e788c467290abafda4c5e7beb",
            "21fc9f3340e240a0ac38da5825dbb918",
            "bd6253b3bbf34141b6ed15997757a95f",
            "15d484d8652e45fa8ee28b716af2b6eb",
            "c33fa6156af940059b24c87ab1e4453a",
            "f399af8466e64de9b3d0483fb0158b60",
            "5ab7a72f4ce3460da808003b8bbdf704",
            "79d4f7cbe1214561bacc509f8f53a7aa",
            "7a0ef80fc4ef4d6b86e9fbf4c7f78572",
            "ad4cb6c9409a4f1d8ef7aeeead52e62a",
            "b640087f960743f99f91a1d89be52274",
            "672262d9b39b45348571c4b667bf5851",
            "c6e75ba4a7804f3099f7fa77f2f2d1bb",
            "301ada6710fb4a6c847355be4bcafe92",
            "587bb4d179e74de9b7da180a0e219f93",
            "79c4640a2ab14ed8a2e6ebc676f017df",
            "745fb1354a354735adf68af9e78766e0",
            "d5c44aa291a347b4b55f6bfbb258f257",
            "d35312967e2b4f5e86c2b81ad187e574",
            "9cb0cbabec294e19aa53bc38c917bb15",
            "bcf296aa80204b5891c634b6f0e2e134",
            "db4b7564f1924846a538f8a6b94d6cb6",
            "841b4897920b472b9b59466adc4dca04",
            "39cd703e0ed24c1f82cefd8564cf372b",
            "b8cab99b0df34841af98f6f98a0a9c6f"
          ]
        },
        "id": "D8GNzGGt_3N5",
        "outputId": "94c09c7a-ef95-46ef-e398-02c813180b02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting CNN/DailyMail Preprocessing...\n",
            "==================================================\n",
            "📂 Loading CNN/DailyMail dataset...\n",
            "✅ Dataset loaded from disk with splits: ['train', 'validation', 'test']\n",
            "  train: 287113 samples\n",
            "  validation: 13368 samples\n",
            "  test: 11490 samples\n",
            "\n",
            "📊 Analyzing text statistics...\n",
            "📊 Analyzing text statistics...\n",
            "Analyzing 10000 samples from training set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing samples: 100%|██████████| 10000/10000 [01:12<00:00, 137.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 Text Statistics:\n",
            "  Article length - Mean: 791.6, Median: 721.0, Range: 25-2161\n",
            "  Summary length - Mean: 55.2, Median: 52.0, Range: 8-324\n",
            "  Compression ratio - Mean: 0.085, Median: 0.073\n",
            "\n",
            "📚 Analyzing vocabulary...\n",
            "📚 Creating vocabulary analysis...\n",
            "Analyzing vocabulary from 10000 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing vocabulary: 100%|██████████| 10000/10000 [01:13<00:00, 136.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Total words processed: 3,980,084\n",
            "  Unique words: 93,835\n",
            "  Most common word: 'said' (46940 occurrences)\n",
            "\n",
            "🔤 Tokenizing and preprocessing...\n",
            "🔤 Tokenizing text with bert-base-uncased...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eba0e8058a4544a499063de85028bbc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6283136b77749719500850655bdb6ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15d484d8652e45fa8ee28b716af2b6eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "587bb4d179e74de9b7da180a0e219f93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing train: 100%|██████████| 1000/1000 [00:05<00:00, 195.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ train: 1000 samples processed\n",
            "Processing validation split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing validation: 100%|██████████| 13368/13368 [01:50<00:00, 120.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ validation: 13368 samples processed\n",
            "Processing test split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing test: 100%|██████████| 11490/11490 [01:43<00:00, 110.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ test: 11490 samples processed\n",
            "📝 Creating sample examples...\n",
            "✅ 5 sample examples saved to /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/results/visualizations/generation\n",
            "💾 Saving processed data...\n",
            "❌ Error during preprocessing: Object of type int64 is not JSON serializable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1572470490.py\", line 572, in main\n",
            "    save_processed_data(processed_splits, tokenizer, text_stats, vocab_info)\n",
            "  File \"/tmp/ipython-input-1572470490.py\", line 472, in save_processed_data\n",
            "    json.dump(metadata, f, indent=2)\n",
            "  File \"/usr/lib/python3.12/json/__init__.py\", line 179, in dump\n",
            "    for chunk in iterable:\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/json/encoder.py\", line 432, in _iterencode\n",
            "    yield from _iterencode_dict(o, _current_indent_level)\n",
            "  File \"/usr/lib/python3.12/json/encoder.py\", line 406, in _iterencode_dict\n",
            "    yield from chunks\n",
            "  File \"/usr/lib/python3.12/json/encoder.py\", line 406, in _iterencode_dict\n",
            "    yield from chunks\n",
            "  File \"/usr/lib/python3.12/json/encoder.py\", line 406, in _iterencode_dict\n",
            "    yield from chunks\n",
            "  File \"/usr/lib/python3.12/json/encoder.py\", line 439, in _iterencode\n",
            "    o = _default(o)\n",
            "        ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/json/encoder.py\", line 180, in default\n",
            "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
            "TypeError: Object of type int64 is not JSON serializable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Preprocessing failed! Check the error messages above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CNN/DailyMail Dataset Preprocessing\n",
        "Handles text summarization data preparation\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Project paths - FIXED\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/PaperSense-Intelligent-Document-Platform\"\n",
        "RAW_DATA_DIR = os.path.join(PROJECT_ROOT, \"datasets\", \"raw\", \"cnn_dailymail\")\n",
        "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT, \"datasets\", \"processed\", \"cnn_dailymail\")\n",
        "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\", \"visualizations\", \"generation\")\n",
        "\n",
        "def ensure_dir(path):\n",
        "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def convert_numpy_types(obj):\n",
        "    \"\"\"Recursively convert numpy types to Python native types for JSON serialization\"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_types(item) for item in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(convert_numpy_types(item) for item in obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def download_cnn_dailymail_dataset():\n",
        "    \"\"\"Download and save CNN/DailyMail dataset\"\"\"\n",
        "    print(\"📥 Downloading CNN/DailyMail dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Download dataset from Hugging Face\n",
        "        dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "        # Create directory and save\n",
        "        ensure_dir(os.path.dirname(RAW_DATA_DIR))\n",
        "        dataset.save_to_disk(RAW_DATA_DIR)\n",
        "\n",
        "        print(f\"✅ Dataset downloaded and saved to: {RAW_DATA_DIR}\")\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_cnn_dailymail_dataset():\n",
        "    \"\"\"Load CNN/DailyMail dataset from disk or download if not exists\"\"\"\n",
        "    print(\"📂 Loading CNN/DailyMail dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Try to load from disk first\n",
        "        if os.path.exists(RAW_DATA_DIR) and os.listdir(RAW_DATA_DIR):\n",
        "            dataset = load_from_disk(RAW_DATA_DIR)\n",
        "            print(f\"✅ Dataset loaded from disk with splits: {list(dataset.keys())}\")\n",
        "        else:\n",
        "            print(\"📥 Dataset not found on disk, downloading...\")\n",
        "            dataset = download_cnn_dailymail_dataset()\n",
        "            if dataset is None:\n",
        "                return None\n",
        "\n",
        "        # Print basic info\n",
        "        for split_name, split_data in dataset.items():\n",
        "            print(f\"  {split_name}: {len(split_data)} samples\")\n",
        "\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading dataset: {e}\")\n",
        "        print(\"🔄 Attempting to download dataset...\")\n",
        "        return download_cnn_dailymail_dataset()\n",
        "\n",
        "def analyze_text_statistics(dataset):\n",
        "    \"\"\"Analyze text length and content statistics\"\"\"\n",
        "    print(\"📊 Analyzing text statistics...\")\n",
        "\n",
        "    ensure_dir(RESULTS_DIR)\n",
        "\n",
        "    # Collect statistics\n",
        "    article_lengths = []\n",
        "    summary_lengths = []\n",
        "    article_sent_counts = []\n",
        "    summary_sent_counts = []\n",
        "\n",
        "    # Sample from train set for analysis\n",
        "    train_data = dataset['train']\n",
        "    sample_size = min(10000, len(train_data))  # Sample for faster analysis\n",
        "\n",
        "    # Fix: Convert numpy integers to Python integers\n",
        "    sample_indices = np.random.choice(len(train_data), sample_size, replace=False)\n",
        "    sample_indices = [int(idx) for idx in sample_indices]  # Convert to Python int\n",
        "\n",
        "    print(f\"Analyzing {sample_size} samples from training set...\")\n",
        "\n",
        "    for idx in tqdm(sample_indices, desc=\"Analyzing samples\"):\n",
        "        article = train_data[idx]['article']\n",
        "        summary = train_data[idx]['highlights']\n",
        "\n",
        "        # Word counts\n",
        "        article_words = len(word_tokenize(article))\n",
        "        summary_words = len(word_tokenize(summary))\n",
        "        article_lengths.append(article_words)\n",
        "        summary_lengths.append(summary_words)\n",
        "\n",
        "        # Sentence counts\n",
        "        article_sents = len(sent_tokenize(article))\n",
        "        summary_sents = len(sent_tokenize(summary))\n",
        "        article_sent_counts.append(article_sents)\n",
        "        summary_sent_counts.append(summary_sents)\n",
        "\n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Article length distribution\n",
        "    axes[0,0].hist(article_lengths, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "    axes[0,0].set_title('Article Length Distribution (Words)')\n",
        "    axes[0,0].set_xlabel('Number of Words')\n",
        "    axes[0,0].set_ylabel('Frequency')\n",
        "    axes[0,0].axvline(np.mean(article_lengths), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(article_lengths):.0f}')\n",
        "    axes[0,0].legend()\n",
        "\n",
        "    # Summary length distribution\n",
        "    axes[0,1].hist(summary_lengths, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "    axes[0,1].set_title('Summary Length Distribution (Words)')\n",
        "    axes[0,1].set_xlabel('Number of Words')\n",
        "    axes[0,1].set_ylabel('Frequency')\n",
        "    axes[0,1].axvline(np.mean(summary_lengths), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(summary_lengths):.0f}')\n",
        "    axes[0,1].legend()\n",
        "\n",
        "    # Length ratio distribution\n",
        "    ratios = [s/a if a > 0 else 0 for a, s in zip(article_lengths, summary_lengths)]\n",
        "    axes[0,2].hist(ratios, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
        "    axes[0,2].set_title('Summary/Article Length Ratio')\n",
        "    axes[0,2].set_xlabel('Ratio')\n",
        "    axes[0,2].set_ylabel('Frequency')\n",
        "    axes[0,2].axvline(np.mean(ratios), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(ratios):.3f}')\n",
        "    axes[0,2].legend()\n",
        "\n",
        "    # Article sentence count\n",
        "    axes[1,0].hist(article_sent_counts, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "    axes[1,0].set_title('Article Sentence Count Distribution')\n",
        "    axes[1,0].set_xlabel('Number of Sentences')\n",
        "    axes[1,0].set_ylabel('Frequency')\n",
        "    axes[1,0].axvline(np.mean(article_sent_counts), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(article_sent_counts):.1f}')\n",
        "    axes[1,0].legend()\n",
        "\n",
        "    # Summary sentence count\n",
        "    axes[1,1].hist(summary_sent_counts, bins=15, alpha=0.7, color='brown', edgecolor='black')\n",
        "    axes[1,1].set_title('Summary Sentence Count Distribution')\n",
        "    axes[1,1].set_xlabel('Number of Sentences')\n",
        "    axes[1,1].set_ylabel('Frequency')\n",
        "    axes[1,1].axvline(np.mean(summary_sent_counts), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(summary_sent_counts):.1f}')\n",
        "    axes[1,1].legend()\n",
        "\n",
        "    # Correlation plot\n",
        "    axes[1,2].scatter(article_lengths, summary_lengths, alpha=0.5, s=1)\n",
        "    axes[1,2].set_title('Article vs Summary Length')\n",
        "    axes[1,2].set_xlabel('Article Length (Words)')\n",
        "    axes[1,2].set_ylabel('Summary Length (Words)')\n",
        "\n",
        "    # Add correlation coefficient\n",
        "    correlation = np.corrcoef(article_lengths, summary_lengths)[0,1]\n",
        "    axes[1,2].text(0.05, 0.95, f'Correlation: {correlation:.3f}',\n",
        "                  transform=axes[1,2].transAxes, verticalalignment='top',\n",
        "                  bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'text_statistics.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Print statistics - Convert numpy types to Python types\n",
        "    stats = {\n",
        "        'article_length': {\n",
        "            'mean': float(np.mean(article_lengths)),\n",
        "            'std': float(np.std(article_lengths)),\n",
        "            'median': float(np.median(article_lengths)),\n",
        "            'min': int(np.min(article_lengths)),\n",
        "            'max': int(np.max(article_lengths))\n",
        "        },\n",
        "        'summary_length': {\n",
        "            'mean': float(np.mean(summary_lengths)),\n",
        "            'std': float(np.std(summary_lengths)),\n",
        "            'median': float(np.median(summary_lengths)),\n",
        "            'min': int(np.min(summary_lengths)),\n",
        "            'max': int(np.max(summary_lengths))\n",
        "        },\n",
        "        'compression_ratio': {\n",
        "            'mean': float(np.mean(ratios)),\n",
        "            'std': float(np.std(ratios)),\n",
        "            'median': float(np.median(ratios))\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(f\"📈 Text Statistics:\")\n",
        "    print(f\"  Article length - Mean: {stats['article_length']['mean']:.1f}, \"\n",
        "          f\"Median: {stats['article_length']['median']:.1f}, \"\n",
        "          f\"Range: {stats['article_length']['min']}-{stats['article_length']['max']}\")\n",
        "    print(f\"  Summary length - Mean: {stats['summary_length']['mean']:.1f}, \"\n",
        "          f\"Median: {stats['summary_length']['median']:.1f}, \"\n",
        "          f\"Range: {stats['summary_length']['min']}-{stats['summary_length']['max']}\")\n",
        "    print(f\"  Compression ratio - Mean: {stats['compression_ratio']['mean']:.3f}, \"\n",
        "          f\"Median: {stats['compression_ratio']['median']:.3f}\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove special characters but keep punctuation\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', '', text)\n",
        "\n",
        "    # Fix common issues\n",
        "    text = text.replace(' .', '.')\n",
        "    text = text.replace(' ,', ',')\n",
        "    text = text.replace('( ', '(')\n",
        "    text = text.replace(' )', ')')\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def create_vocabulary_analysis(dataset, max_vocab_size=50000):\n",
        "    \"\"\"Analyze vocabulary and create word frequency statistics\"\"\"\n",
        "    print(\"📚 Creating vocabulary analysis...\")\n",
        "\n",
        "    ensure_dir(RESULTS_DIR)\n",
        "    ensure_dir(PROCESSED_DATA_DIR)  # Ensure this exists for vocabulary.json\n",
        "\n",
        "    # Collect text from training set\n",
        "    train_data = dataset['train']\n",
        "    sample_size = min(10000, len(train_data))  # Sample for faster analysis\n",
        "\n",
        "    # Fix: Convert numpy integers to Python integers\n",
        "    sample_indices = np.random.choice(len(train_data), sample_size, replace=False)\n",
        "    sample_indices = [int(idx) for idx in sample_indices]  # Convert to Python int\n",
        "\n",
        "    all_words = []\n",
        "    stopwords_en = set(stopwords.words('english'))\n",
        "\n",
        "    print(f\"Analyzing vocabulary from {sample_size} samples...\")\n",
        "\n",
        "    for idx in tqdm(sample_indices, desc=\"Processing vocabulary\"):\n",
        "        article = train_data[idx]['article']\n",
        "        summary = train_data[idx]['highlights']\n",
        "\n",
        "        # Tokenize and clean\n",
        "        article_words = word_tokenize(clean_text(article.lower()))\n",
        "        summary_words = word_tokenize(clean_text(summary.lower()))\n",
        "\n",
        "        # Filter words\n",
        "        article_words = [w for w in article_words if w.isalpha() and w not in stopwords_en]\n",
        "        summary_words = [w for w in summary_words if w.isalpha() and w not in stopwords_en]\n",
        "\n",
        "        all_words.extend(article_words)\n",
        "        all_words.extend(summary_words)\n",
        "\n",
        "    # Create word frequency distribution\n",
        "    word_freq = Counter(all_words)\n",
        "    most_common = word_freq.most_common(max_vocab_size)\n",
        "\n",
        "    # Visualizations\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Top 30 words\n",
        "    top_30_words = most_common[:30]\n",
        "    words, counts = zip(*top_30_words)\n",
        "\n",
        "    ax1.barh(range(len(words)), counts)\n",
        "    ax1.set_yticks(range(len(words)))\n",
        "    ax1.set_yticklabels(words)\n",
        "    ax1.set_title('Top 30 Most Frequent Words')\n",
        "    ax1.set_xlabel('Frequency')\n",
        "    ax1.invert_yaxis()\n",
        "\n",
        "    # Word frequency distribution (Zipf's law)\n",
        "    ranks = range(1, min(1000, len(most_common)) + 1)\n",
        "    freqs = [count for _, count in most_common[:len(ranks)]]\n",
        "\n",
        "    ax2.loglog(ranks, freqs, 'b-', alpha=0.7)\n",
        "    ax2.set_title('Word Frequency Distribution (Zipf\\'s Law)')\n",
        "    ax2.set_xlabel('Rank')\n",
        "    ax2.set_ylabel('Frequency')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'vocabulary_analysis.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Save vocabulary - Convert to JSON-serializable format\n",
        "    vocab_info = {\n",
        "        'total_words': len(all_words),\n",
        "        'unique_words': len(word_freq),\n",
        "        'most_common': most_common[:1000],  # Save top 1000\n",
        "        'vocabulary_size': len(word_freq)\n",
        "    }\n",
        "\n",
        "    # Convert numpy types to Python types\n",
        "    vocab_info = convert_numpy_types(vocab_info)\n",
        "\n",
        "    with open(os.path.join(PROCESSED_DATA_DIR, 'vocabulary.json'), 'w') as f:\n",
        "        json.dump(vocab_info, f, indent=2)\n",
        "\n",
        "    print(f\"  Total words processed: {len(all_words):,}\")\n",
        "    print(f\"  Unique words: {len(word_freq):,}\")\n",
        "    print(f\"  Most common word: '{most_common[0][0]}' ({most_common[0][1]} occurrences)\")\n",
        "\n",
        "    return vocab_info\n",
        "\n",
        "def tokenize_and_preprocess(dataset, tokenizer_name='bert-base-uncased', max_samples=None):\n",
        "    \"\"\"Tokenize text using a pre-trained tokenizer\"\"\"\n",
        "    print(f\"🔤 Tokenizing text with {tokenizer_name}...\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    except:\n",
        "        print(f\"Warning: Could not load {tokenizer_name}, using basic preprocessing\")\n",
        "        return preprocess_basic_tokenization(dataset, max_samples)\n",
        "\n",
        "    processed_splits = {}\n",
        "\n",
        "    for split_name, split_data in dataset.items():\n",
        "        print(f\"Processing {split_name} split...\")\n",
        "\n",
        "        # Limit samples for testing\n",
        "        if max_samples and split_name == 'train':\n",
        "            indices = list(range(min(max_samples, len(split_data))))  # Convert to list of Python ints\n",
        "        else:\n",
        "            indices = list(range(len(split_data)))  # Convert to list of Python ints\n",
        "\n",
        "        articles = []\n",
        "        summaries = []\n",
        "\n",
        "        for idx in tqdm(indices, desc=f\"Tokenizing {split_name}\"):\n",
        "            article = clean_text(split_data[idx]['article'])\n",
        "            summary = clean_text(split_data[idx]['highlights'])\n",
        "\n",
        "            # Tokenize article (truncate to model max length)\n",
        "            article_tokens = tokenizer(\n",
        "                article,\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                padding=False,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Tokenize summary\n",
        "            summary_tokens = tokenizer(\n",
        "                summary,\n",
        "                max_length=128,\n",
        "                truncation=True,\n",
        "                padding=False,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            articles.append({\n",
        "                'input_ids': article_tokens['input_ids'].squeeze(),\n",
        "                'attention_mask': article_tokens['attention_mask'].squeeze(),\n",
        "                'text': article\n",
        "            })\n",
        "\n",
        "            summaries.append({\n",
        "                'input_ids': summary_tokens['input_ids'].squeeze(),\n",
        "                'attention_mask': summary_tokens['attention_mask'].squeeze(),\n",
        "                'text': summary\n",
        "            })\n",
        "\n",
        "        processed_splits[split_name] = {\n",
        "            'articles': articles,\n",
        "            'summaries': summaries,\n",
        "            'count': len(articles)\n",
        "        }\n",
        "\n",
        "        print(f\"✅ {split_name}: {len(articles)} samples processed\")\n",
        "\n",
        "    return processed_splits, tokenizer\n",
        "\n",
        "def preprocess_basic_tokenization(dataset, max_samples=None):\n",
        "    \"\"\"Basic text preprocessing without transformer tokenizer\"\"\"\n",
        "    print(\"🔤 Performing basic text preprocessing...\")\n",
        "\n",
        "    processed_splits = {}\n",
        "\n",
        "    for split_name, split_data in dataset.items():\n",
        "        print(f\"Processing {split_name} split...\")\n",
        "\n",
        "        # Limit samples for testing\n",
        "        if max_samples and split_name == 'train':\n",
        "            indices = list(range(min(max_samples, len(split_data))))  # Convert to list of Python ints\n",
        "        else:\n",
        "            indices = list(range(len(split_data)))  # Convert to list of Python ints\n",
        "\n",
        "        articles = []\n",
        "        summaries = []\n",
        "\n",
        "        for idx in tqdm(indices, desc=f\"Processing {split_name}\"):\n",
        "            article = clean_text(split_data[idx]['article'])\n",
        "            summary = clean_text(split_data[idx]['highlights'])\n",
        "\n",
        "            # Basic tokenization\n",
        "            article_words = word_tokenize(article.lower())\n",
        "            summary_words = word_tokenize(summary.lower())\n",
        "\n",
        "            articles.append({\n",
        "                'tokens': article_words,\n",
        "                'text': article,\n",
        "                'length': len(article_words)\n",
        "            })\n",
        "\n",
        "            summaries.append({\n",
        "                'tokens': summary_words,\n",
        "                'text': summary,\n",
        "                'length': len(summary_words)\n",
        "            })\n",
        "\n",
        "        processed_splits[split_name] = {\n",
        "            'articles': articles,\n",
        "            'summaries': summaries,\n",
        "            'count': len(articles)\n",
        "        }\n",
        "\n",
        "        print(f\"✅ {split_name}: {len(articles)} samples processed\")\n",
        "\n",
        "    return processed_splits, None\n",
        "\n",
        "def save_processed_data(processed_splits, tokenizer, text_stats, vocab_info):\n",
        "    \"\"\"Save processed CNN/DailyMail data\"\"\"\n",
        "    print(\"💾 Saving processed data...\")\n",
        "\n",
        "    ensure_dir(PROCESSED_DATA_DIR)\n",
        "\n",
        "    # Save processed splits\n",
        "    for split_name, split_data in processed_splits.items():\n",
        "        with open(os.path.join(PROCESSED_DATA_DIR, f\"{split_name}_processed.pkl\"), 'wb') as f:\n",
        "            pickle.dump(split_data, f)\n",
        "\n",
        "    # Save tokenizer if available\n",
        "    if tokenizer:\n",
        "        tokenizer.save_pretrained(os.path.join(PROCESSED_DATA_DIR, 'tokenizer'))\n",
        "\n",
        "    # Save metadata - Convert all numpy types to Python types\n",
        "    metadata = {\n",
        "        'dataset_name': 'CNN/DailyMail',\n",
        "        'task': 'text_summarization',\n",
        "        'splits': {name: data['count'] for name, data in processed_splits.items()},\n",
        "        'tokenizer': tokenizer.name_or_path if tokenizer else 'basic_tokenization',\n",
        "        'text_statistics': convert_numpy_types(text_stats),\n",
        "        'vocabulary_info': {\n",
        "            'total_words': vocab_info['total_words'],\n",
        "            'unique_words': vocab_info['unique_words'],\n",
        "            'vocabulary_size': vocab_info['vocabulary_size']\n",
        "        },\n",
        "        'preprocessing_info': {\n",
        "            'max_article_length': 512,\n",
        "            'max_summary_length': 128,\n",
        "            'text_cleaning': True,\n",
        "            'truncation': True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Ensure all data is JSON serializable\n",
        "    metadata = convert_numpy_types(metadata)\n",
        "\n",
        "    with open(os.path.join(PROCESSED_DATA_DIR, 'metadata.json'), 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Data saved to {PROCESSED_DATA_DIR}\")\n",
        "\n",
        "    # Print summary\n",
        "    total_samples = sum(data['count'] for data in processed_splits.values())\n",
        "    print(f\"\\n📋 Processing Summary:\")\n",
        "    print(f\"  Total samples: {total_samples:,}\")\n",
        "    for split_name, data in processed_splits.items():\n",
        "        print(f\"  {split_name}: {data['count']:,} samples\")\n",
        "\n",
        "def create_sample_examples(processed_splits, num_examples=5):\n",
        "    \"\"\"Create sample examples for visualization\"\"\"\n",
        "    print(\"📝 Creating sample examples...\")\n",
        "\n",
        "    ensure_dir(RESULTS_DIR)\n",
        "\n",
        "    train_data = processed_splits['train']\n",
        "\n",
        "    # Fix: Convert numpy integers to Python integers\n",
        "    sample_indices = np.random.choice(len(train_data['articles']), num_examples, replace=False)\n",
        "    sample_indices = [int(idx) for idx in sample_indices]  # Convert to Python int\n",
        "\n",
        "    examples = []\n",
        "    for idx in sample_indices:\n",
        "        article = train_data['articles'][idx]\n",
        "        summary = train_data['summaries'][idx]\n",
        "\n",
        "        example = {\n",
        "            'index': int(idx),  # Ensure Python int\n",
        "            'article': article['text'][:1000] + '...' if len(article['text']) > 1000 else article['text'],\n",
        "            'summary': summary['text'],\n",
        "            'article_length': len(article['text'].split()) if 'text' in article else article.get('length', 0),\n",
        "            'summary_length': len(summary['text'].split()) if 'text' in summary else summary.get('length', 0)\n",
        "        }\n",
        "        examples.append(example)\n",
        "\n",
        "    # Convert to ensure JSON serializable\n",
        "    examples = convert_numpy_types(examples)\n",
        "\n",
        "    # Save examples as JSON for easy viewing\n",
        "    with open(os.path.join(RESULTS_DIR, 'sample_examples.json'), 'w') as f:\n",
        "        json.dump(examples, f, indent=2)\n",
        "\n",
        "    # Create a formatted text file\n",
        "    with open(os.path.join(RESULTS_DIR, 'sample_examples.txt'), 'w') as f:\n",
        "        f.write(\"CNN/DailyMail Dataset - Sample Examples\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        for i, example in enumerate(examples, 1):\n",
        "            f.write(f\"EXAMPLE {i}:\\n\")\n",
        "            f.write(f\"Article Length: {example['article_length']} words\\n\")\n",
        "            f.write(f\"Summary Length: {example['summary_length']} words\\n\")\n",
        "            f.write(f\"Compression Ratio: {example['summary_length']/example['article_length']:.3f}\\n\\n\")\n",
        "\n",
        "            f.write(\"ARTICLE:\\n\")\n",
        "            f.write(example['article'])\n",
        "            f.write(\"\\n\\nSUMMARY:\\n\")\n",
        "            f.write(example['summary'])\n",
        "            f.write(\"\\n\\n\" + \"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "    print(f\"✅ {num_examples} sample examples saved to {RESULTS_DIR}\")\n",
        "    return examples\n",
        "\n",
        "def main(max_samples=None, tokenizer_name='bert-base-uncased', create_visualizations=True):\n",
        "    \"\"\"Main CNN/DailyMail preprocessing function\"\"\"\n",
        "    print(\"🚀 Starting CNN/DailyMail Preprocessing...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create directories\n",
        "    ensure_dir(PROCESSED_DATA_DIR)\n",
        "    ensure_dir(RESULTS_DIR)\n",
        "\n",
        "    # Load dataset (will download if not exists)\n",
        "    dataset = load_cnn_dailymail_dataset()\n",
        "    if dataset is None:\n",
        "        print(\"❌ Failed to load dataset. Please check your internet connection.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Analyze text statistics\n",
        "        if create_visualizations:\n",
        "            print(\"\\n📊 Analyzing text statistics...\")\n",
        "            text_stats = analyze_text_statistics(dataset)\n",
        "\n",
        "            print(\"\\n📚 Analyzing vocabulary...\")\n",
        "            vocab_info = create_vocabulary_analysis(dataset)\n",
        "        else:\n",
        "            text_stats = {}\n",
        "            vocab_info = {'total_words': 0, 'unique_words': 0, 'vocabulary_size': 0}\n",
        "\n",
        "        # Tokenize and preprocess\n",
        "        print(\"\\n🔤 Tokenizing and preprocessing...\")\n",
        "        processed_splits, tokenizer = tokenize_and_preprocess(\n",
        "            dataset, tokenizer_name, max_samples\n",
        "        )\n",
        "\n",
        "        # Create sample examples\n",
        "        if create_visualizations and processed_splits:\n",
        "            create_sample_examples(processed_splits)\n",
        "\n",
        "        # Save processed data\n",
        "        if processed_splits:\n",
        "            save_processed_data(processed_splits, tokenizer, text_stats, vocab_info)\n",
        "            print(\"🎉 CNN/DailyMail preprocessing completed successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ No data was processed!\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during preprocessing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Quick download function for immediate use\n",
        "def quick_download():\n",
        "    \"\"\"Quick function to just download the dataset\"\"\"\n",
        "    print(\"🚀 Quick Download: CNN/DailyMail Dataset\")\n",
        "    ensure_dir(os.path.dirname(RAW_DATA_DIR))\n",
        "    dataset = download_cnn_dailymail_dataset()\n",
        "    return dataset is not None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For testing, limit samples per split\n",
        "    # Remove max_samples parameter for full processing\n",
        "    success = main(\n",
        "        max_samples=1000,  # Limit for testing - set to None for full dataset\n",
        "        tokenizer_name='bert-base-uncased',\n",
        "        create_visualizations=True\n",
        "    )\n",
        "\n",
        "    if success:\n",
        "        print(\"\\n✅ Preprocessing completed! You can now use the processed data for training.\")\n",
        "        print(f\"📁 Raw data: {RAW_DATA_DIR}\")\n",
        "        print(f\"📁 Processed data: {PROCESSED_DATA_DIR}\")\n",
        "        print(f\"📁 Visualizations: {RESULTS_DIR}\")\n",
        "    else:\n",
        "        print(\"\\n❌ Preprocessing failed! Check the error messages above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEorKZajBNB6",
        "outputId": "d9093e12-bb83-4703-cdbf-d1631667504a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting CNN/DailyMail Preprocessing...\n",
            "==================================================\n",
            "📂 Loading CNN/DailyMail dataset...\n",
            "✅ Dataset loaded from disk with splits: ['train', 'validation', 'test']\n",
            "  train: 287113 samples\n",
            "  validation: 13368 samples\n",
            "  test: 11490 samples\n",
            "\n",
            "📊 Analyzing text statistics...\n",
            "📊 Analyzing text statistics...\n",
            "Analyzing 10000 samples from training set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing samples: 100%|██████████| 10000/10000 [01:16<00:00, 130.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 Text Statistics:\n",
            "  Article length - Mean: 782.3, Median: 718.0, Range: 49-2181\n",
            "  Summary length - Mean: 54.5, Median: 51.0, Range: 7-426\n",
            "  Compression ratio - Mean: 0.085, Median: 0.073\n",
            "\n",
            "📚 Analyzing vocabulary...\n",
            "📚 Creating vocabulary analysis...\n",
            "Analyzing vocabulary from 10000 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing vocabulary: 100%|██████████| 10000/10000 [01:02<00:00, 160.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Total words processed: 3,984,583\n",
            "  Unique words: 94,106\n",
            "  Most common word: 'said' (47196 occurrences)\n",
            "\n",
            "🔤 Tokenizing and preprocessing...\n",
            "🔤 Tokenizing text with bert-base-uncased...\n",
            "Processing train split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing train: 100%|██████████| 1000/1000 [00:05<00:00, 197.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ train: 1000 samples processed\n",
            "Processing validation split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing validation: 100%|██████████| 13368/13368 [01:47<00:00, 124.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ validation: 13368 samples processed\n",
            "Processing test split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing test: 100%|██████████| 11490/11490 [01:42<00:00, 111.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ test: 11490 samples processed\n",
            "📝 Creating sample examples...\n",
            "✅ 5 sample examples saved to /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/results/visualizations/generation\n",
            "💾 Saving processed data...\n",
            "✅ Data saved to /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/datasets/processed/cnn_dailymail\n",
            "\n",
            "📋 Processing Summary:\n",
            "  Total samples: 25,858\n",
            "  train: 1,000 samples\n",
            "  validation: 13,368 samples\n",
            "  test: 11,490 samples\n",
            "🎉 CNN/DailyMail preprocessing completed successfully!\n",
            "\n",
            "✅ Preprocessing completed! You can now use the processed data for training.\n",
            "📁 Raw data: /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/datasets/raw/cnn_dailymail\n",
            "📁 Processed data: /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/datasets/processed/cnn_dailymail\n",
            "📁 Visualizations: /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/results/visualizations/generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohV8LlppFBNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}